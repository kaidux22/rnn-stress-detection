{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmRADIBLVGFU",
        "outputId": "ceaa74d1-5cb8-4635-8a37-929a81eaa4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXtGr4dLdIpi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rd\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle\n",
        "import time\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import traceback\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Generate permutations\n",
        "gen = torch.Generator()\n",
        "gen.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt2k7wqDaGkr"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Thesis/Datasets/multiclass\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahbyMYHZJodb"
      },
      "source": [
        "#Бинарная классификация (нейтральное состояние/состояние стресса)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMwvTRXlUQPp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Модель классификации с одним LSTM-слоем\n",
        "\"\"\"\n",
        "class LSTMStressBinDetector(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm1(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sig(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19aO-mtZJss7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция загрузки сгенерированных датасетов\n",
        "Используется стратификация и перемешивание. Выборка делится на 85/15 для тренировочной и тестовой выборки соответственно\n",
        "\"\"\"\n",
        "def data_processing_binclass(extract_path, window, step):\n",
        "  if step == 1:\n",
        "    path = f\"{extract_path}/chest_dataset.pkl\"\n",
        "  else:\n",
        "    path = f\"{extract_path}/chest_dataset_{window}_{step}.pkl\"\n",
        "\n",
        "  try:\n",
        "    with open(path, 'rb') as file:\n",
        "      dataset = pickle.load(file, encoding='latin1')\n",
        "  except FileNotFoundError:\n",
        "    return None\n",
        "\n",
        "  X_train, Y_train, X_test, Y_test = None, None, None, None\n",
        "  for key in dataset:\n",
        "    binclass_mask = np.where(dataset[key]['label'] != 2)\n",
        "    y = dataset[key]['label'][binclass_mask]\n",
        "    arrays = train_test_split(dataset[key]['data'][binclass_mask], y, random_state=42, test_size=0.15, stratify=y)\n",
        "    X_train = np.concatenate((arrays[0], X_train), axis=0) if X_train is not None else arrays[0]\n",
        "    X_test = np.concatenate((arrays[1], X_test), axis=0) if X_test is not None else arrays[1]\n",
        "    Y_train = np.concatenate((arrays[2], Y_train), axis=0) if Y_train is not None else arrays[2]\n",
        "    Y_test = np.concatenate((arrays[3], Y_test), axis=0) if Y_test is not None else arrays[3]\n",
        "\n",
        "  X_train, Y_train = shuffle(X_train, Y_train, random_state=42)\n",
        "  X_test, Y_test = shuffle(X_test, Y_test, random_state=42)\n",
        "\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "  Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "  Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "  # Для обучающего набора\n",
        "  train_mask = ~torch.isnan(X_train).any(dim=(1, 2))\n",
        "  X_train_clean = X_train[train_mask]\n",
        "  Y_train_clean = Y_train[train_mask]\n",
        "\n",
        "  # Для тестового набора\n",
        "  test_mask = ~torch.isnan(X_test).any(dim=(1, 2))\n",
        "  X_test_clean = X_test[test_mask]\n",
        "  Y_test_clean = Y_test[test_mask]\n",
        "\n",
        "  return X_train_clean, Y_train_clean, X_test_clean, Y_test_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO4VPHdMGkib"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция обучения и валидации на одном фолде\n",
        "\"\"\"\n",
        "def binclass_train_model(X_train, Y_train, X_val, Y_val, input_size, num_epochs=300, patience=30):\n",
        "    model = LSTMStressBinDetector(input_size)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, min_lr=1e-6)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs.view(-1), Y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.view(-1), Y_val)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Logging\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        # Save best model for this fold\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            early_stop_counter = 0\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        # Early stopping for this fold\n",
        "        if early_stop_counter >= patience and epoch > 50:\n",
        "            print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "    # Load best weights for this fold\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "\"\"\"\n",
        "Функция кросс-валидирования с помощью 10-fold\n",
        "\"\"\"\n",
        "def binclass_training_with_cv(X_train, Y_train, X_test, Y_test, n_splits=10, num_epochs=300, patience=30):\n",
        "    input_size = X_train.shape[2]\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, Y_train)):\n",
        "        print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "        # Split data\n",
        "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "        Y_fold_train, Y_fold_val = Y_train[train_idx], Y_train[val_idx]\n",
        "\n",
        "        # Train with early stopping per fold\n",
        "        model, train_loss, val_loss = binclass_train_model(\n",
        "            X_fold_train, Y_fold_train,\n",
        "            X_fold_val, Y_fold_val,\n",
        "            input_size,\n",
        "            num_epochs=num_epochs,\n",
        "            patience=patience\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set (optional per fold)\n",
        "        fold_test_metrics = binclass_evaluate(model, X_test, Y_test)\n",
        "        fold_metrics.append(fold_test_metrics)\n",
        "        print(f\"Fold {fold + 1} Test Metrics:\", fold_test_metrics)\n",
        "\n",
        "    return fold_metrics\n",
        "\n",
        "\"\"\"\n",
        "Функция оценка качества обучения\n",
        "\"\"\"\n",
        "def binclass_evaluate(model, X, Y, plot_cm=True, class_names=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        preds = (outputs > 0.5).float().cpu().numpy()\n",
        "        probs = outputs.cpu().numpy()\n",
        "        labels = Y.cpu().numpy()\n",
        "\n",
        "    # Вычисляем Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'precision': precision_score(labels, preds),\n",
        "        'recall': recall_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds),\n",
        "        'auroc': roc_auc_score(labels, probs),\n",
        "        \"confusion_matrix\": cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r41zoUvovSWI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция запуска эксперимента для одного датасета\n",
        "\"\"\"\n",
        "def run_experiment(path, window, timestep):\n",
        "    ans = data_processing_binclass(path, window, timestep)\n",
        "\n",
        "    if ans is None:\n",
        "      return None\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = ans\n",
        "    # Записываем в лог параметры\n",
        "    with open(log_file, \"a\") as f:\n",
        "      f.write(f\"\\n=== Window: {window}, Timestep: {timestep} ===\\n\")\n",
        "\n",
        "    print(f\"\\nProcessing window={window}, timestep={timestep}\")\n",
        "\n",
        "    try:\n",
        "      metrics = binclass_training_with_cv(X_train, Y_train, X_test, Y_test, n_splits=10)\n",
        "\n",
        "      # Записываем в лог метрики\n",
        "      with open(log_file, \"a\") as f:\n",
        "        f.write(f\"Metrics: {metrics}\\n\")\n",
        "        f.write(f\"Accuracy: {np.mean([fold['accuracy'] for fold in metrics]):.4f}, Precision: {np.mean([fold['precision'] for fold in metrics]):.4f}, Recall: {np.mean([fold['recall'] for fold in metrics]):.4f},  F1: {np.mean([fold['f1'] for fold in metrics]):.4f}, AUROC: {np.mean([fold['auroc'] for fold in metrics]):.4f}\\n\")\n",
        "\n",
        "        print(f\"Final Test Metrics: {np.mean([fold['accuracy'] for fold in metrics]):.4f}, Precision: {np.mean([fold['precision'] for fold in metrics]):.4f}, Recall: {np.mean([fold['recall'] for fold in metrics]):.4f},  F1: {np.mean([fold['f1'] for fold in metrics]):.4f}, AUROC: {np.mean([fold['auroc'] for fold in metrics]):.4f}\\n\")\n",
        "      return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "      with open(log_file, \"a\") as f:\n",
        "        f.write(f\"Error: {str(e)}\\n\")\n",
        "        print(f\"Error for window={window}, timestep={timestep}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLT6VqklMsgp"
      },
      "outputs": [],
      "source": [
        "# Создаем папку для результатов (если нет)\n",
        "!mkdir -p /content/drive/MyDrive/Thesis/Results\n",
        "\n",
        "# Инициализируем переменные для сохранения\n",
        "all_steps = None\n",
        "all_results = None\n",
        "log_file = \"/content/drive/MyDrive/Thesis/Results/chest_binclass_log_v1.txt\"\n",
        "\n",
        "# Открываем файл для записи логов\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"Training Log - \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
        "\n",
        "chest_binclass_dict = {}\n",
        "\n",
        "chest_binclass_dict[(15, 1)] = run_experiment(path, 0, 1)\n",
        "chest_binclass_dict[(30, 1)] = chest_binclass_dict[(15, 1)]\n",
        "chest_binclass_dict[(45, 1)] = chest_binclass_dict[(15, 1)]\n",
        "\n",
        "for window in range(15, 46, 15):\n",
        "    for timestep in range(2, 9):\n",
        "        chest_binclass_dict[(window, timestep)] = run_experiment(path, window, timestep)\n",
        "\n",
        "with open('/content/drive/MyDrive/Thesis/Results/chest_binclass_v1.pkl', 'wb') as f:\n",
        "    pickle.dump(chest_binclass_dict, f)\n",
        "\n",
        "# Выводим информацию о сохраненных файлах\n",
        "print(\"\\nSaved files:\")\n",
        "print(f\"- Training logs: {log_file}\")\n",
        "print(f\"- All results: /content/drive/MyDrive/Thesis/Results/chest_binclass_v1.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_3Ji7p7oYcw"
      },
      "source": [
        "#Многоклассовая классификация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rAHbS92nJ-4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Модель классификации с одним LSTM-слоем\n",
        "\"\"\"\n",
        "class LSTMStressMultiDetector1h(nn.Module):\n",
        "    def __init__(self, input_size=15, hidden_size=200, num_layers=1, num_classes=3):\n",
        "        super().__init__()\n",
        "        # Первый LSTM слой\n",
        "        self.first_lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.first_lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJruWynzPFeK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Модель классификации с 4 LSTM-слоями, 3 из которых скрытые\n",
        "\"\"\"\n",
        "class LSTMStressMultiDetector4h(nn.Module):\n",
        "    def __init__(self, input_size=15, hidden_size=200, num_layers=4, num_classes=3):\n",
        "        super().__init__()\n",
        "        cur_hidden_size = hidden_size\n",
        "        # Первый LSTM слой\n",
        "        self.first_lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=cur_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        cur_hidden_size -= 50\n",
        "\n",
        "        self.hid_lstm = []\n",
        "        self.dropout = []\n",
        "        for _ in range(num_layers - 2):\n",
        "          self.hid_lstm.append(\n",
        "              nn.LSTM(\n",
        "            input_size=cur_hidden_size + 50,\n",
        "            hidden_size=cur_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "            )\n",
        "          )\n",
        "\n",
        "          self.dropout.append(\n",
        "              nn.Dropout(0.2)\n",
        "          )\n",
        "\n",
        "          cur_hidden_size -= 50\n",
        "\n",
        "        self.last_lstm = nn.LSTM(\n",
        "            input_size=cur_hidden_size + 50,\n",
        "            hidden_size=cur_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(cur_hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.first_lstm(x)\n",
        "\n",
        "        for i in range(len(self.hid_lstm)):\n",
        "          out, _ = self.hid_lstm[i](out)\n",
        "          out = self.dropout[i](out)\n",
        "\n",
        "        out, _ = self.last_lstm(out)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5SCwvFynoD-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция загрузки сгенерированных датасетов\n",
        "Используется стратификация и перемешивание. Выборка делится на 85/15 для тренировочной и тестовой выборки соответственно\n",
        "\"\"\"\n",
        "def data_processing_multiclass(extract_path, window, step, class_number=3):\n",
        "    if step == 1:\n",
        "      path = f\"{extract_path}/chest_acc_dataset.pkl\"\n",
        "    else:\n",
        "      path = f\"{extract_path}/chest_acc_dataset_{window}_{step}.pkl\"\n",
        "    try:\n",
        "        with open(path, 'rb') as file:\n",
        "            dataset = pickle.load(file, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = None, None, None, None\n",
        "    for key in dataset:\n",
        "        y = np.eye(class_number)[dataset[key]['label'].astype(int)]\n",
        "        arrays = train_test_split(dataset[key]['data'], y, random_state=42, test_size=0.15, stratify=y.argmax(axis=1))\n",
        "        X_train = np.concatenate((X_train, arrays[0]), axis=0) if X_train is not None else arrays[0]\n",
        "        X_test = np.concatenate((X_test, arrays[1]), axis=0) if X_test is not None else arrays[1]\n",
        "        Y_train = np.concatenate((Y_train, arrays[2]), axis=0) if Y_train is not None else arrays[2]\n",
        "        Y_test = np.concatenate((Y_test, arrays[3]), axis=0) if Y_test is not None else arrays[3]\n",
        "\n",
        "    X_train, Y_train = shuffle(X_train, Y_train, random_state=42)\n",
        "    X_test, Y_test = shuffle(X_test, Y_test, random_state=42)\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "    # Для обучающего набора\n",
        "    train_mask = ~torch.isnan(X_train).any(dim=(1, 2))\n",
        "    X_train_clean = X_train[train_mask]\n",
        "    Y_train_clean = Y_train[train_mask]\n",
        "\n",
        "    # Для тестового набора\n",
        "    test_mask = ~torch.isnan(X_test).any(dim=(1, 2))\n",
        "    X_test_clean = X_test[test_mask]\n",
        "    Y_test_clean = Y_test[test_mask]\n",
        "\n",
        "    return X_train_clean, Y_train_clean, X_test_clean, Y_test_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHVM_A0Iob2g"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция обучения и валидации на одном фолде\n",
        "\"\"\"\n",
        "def multiclass_train_model(X_train, Y_train, X_val, Y_val, input_size, num_epochs=300, patience=30, model_type=\"LSTM1\"):\n",
        "    if model_type == \"LSTM1\":\n",
        "      model = LSTMStressMultiDetector1h(input_size, num_classes=3)\n",
        "    elif model_type == \"LSTM4\":\n",
        "      model = LSTMStressMultiDetector4h(input_size, num_classes=3)\n",
        "    else:\n",
        "      raise ValueError(f\"There no model with type {model_type}\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Изменяем функцию потерь на CrossEntropyLoss (не нужно softmax в модели)\n",
        "    class_counts = torch.bincount(Y_train.argmax(dim=1))\n",
        "    class_weights = 1. / class_counts.float()\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, min_lr=1e-6)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, Y_train.argmax(dim=1))  # Используем argmax для классов\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, Y_val.argmax(dim=1))\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Logging\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        # Save best model for this fold\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            early_stop_counter = 0\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        # Early stopping for this fold\n",
        "        if early_stop_counter >= patience and epoch > 50:\n",
        "            print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "    # Load best weights for this fold\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_losses, val_losses, epoch\n",
        "\n",
        "\"\"\"\n",
        "Функция оценка качества обучения\n",
        "\"\"\"\n",
        "def multiclass_evaluate(model, X, Y, plot_cm=True, class_names=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        logits = outputs.cpu().numpy()\n",
        "        labels = Y.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "    y_true_bin = label_binarize(labels, classes=np.unique(labels))\n",
        "\n",
        "    # Вычисляем Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'precision': precision_score(labels, preds, average='weighted'),\n",
        "        'recall': recall_score(labels, preds, average='weighted'),\n",
        "        'f1': f1_score(labels, preds, average='weighted'),\n",
        "        'auroc': roc_auc_score(y_true_bin, probs, multi_class='ovr', average='weighted'),\n",
        "        'confusion_matrix': cm  # Возвращаем матрицу для дальнейшего анализа\n",
        "    }\n",
        "\n",
        "\"\"\"\n",
        "Функция кросс-валидирования с помощью 10-fold\n",
        "\"\"\"\n",
        "def multiclass_training_with_cv(X_train, Y_train, X_test, Y_test, n_splits=5, num_epochs=300, patience=30, model_type=\"LSTM1\"):\n",
        "    input_size = X_train.shape[2]\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    fold_metrics = []\n",
        "    fold_epochs = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, Y_train.argmax(dim=1))):  # Используем argmax для стратификации\n",
        "        print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "        # Split data\n",
        "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "        Y_fold_train, Y_fold_val = Y_train[train_idx], Y_train[val_idx]\n",
        "\n",
        "        # Train with early stopping per fold\n",
        "        model, train_loss, val_loss, epoch = multiclass_train_model(\n",
        "            X_fold_train, Y_fold_train,\n",
        "            X_fold_val, Y_fold_val,\n",
        "            input_size,\n",
        "            num_epochs=num_epochs,\n",
        "            patience=patience,\n",
        "            model_type=model_type\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set (optional per fold)\n",
        "        class_names = ['Baseline', 'Stress', 'Amusement']\n",
        "        fold_test_metrics = multiclass_evaluate(model, X_test, Y_test, plot_cm=True, class_names=class_names)\n",
        "        fold_metrics.append(fold_test_metrics)\n",
        "        fold_epochs.append(epoch)\n",
        "        print(f\"Fold {fold + 1} Test Metrics:\", fold_test_metrics)\n",
        "\n",
        "    return fold_metrics, fold_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mhNHfTLfIXW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Функция запуска эксперимента для одного датасета\n",
        "\"\"\"\n",
        "def run_multiclass_experiment(path, window, timestep, model_type=\"LSTM1\"):\n",
        "    ans = data_processing_multiclass(path, window, timestep)\n",
        "\n",
        "    if not ans:\n",
        "      return None\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = ans\n",
        "\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(f\"\\n=== Window: {window}, Timestep: {timestep} ===\\n\")\n",
        "\n",
        "    print(f\"\\nProcessing window={window}, timestep={timestep}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "      metrics, epochs = multiclass_training_with_cv(X_train, Y_train, X_test, Y_test, n_splits=10, model_type=model_type)\n",
        "\n",
        "      elapsed = time.time() - start_time\n",
        "\n",
        "      with open(log_file, \"a\") as f:\n",
        "        f.write(f\"Time: {elapsed:.2f} sec\\n\")\n",
        "        f.write(f\"Epochs: {', '.join(map(str, epochs))}\")\n",
        "        f.write(f\"Metrics: {metrics}\\n\")\n",
        "        print(f\"Class distribution (train): {np.unique(Y_train.argmax(dim=1), return_counts=True)}\")\n",
        "        print(f\"Class distribution (test): {np.unique(Y_test.argmax(dim=1), return_counts=True)}\")\n",
        "\n",
        "        print(\"Final Test Metrics:\", metrics)\n",
        "        print(\"Final epoch's number\")\n",
        "\n",
        "    except Exception as e:\n",
        "      with open(log_file, \"a\") as f:\n",
        "          f.write(f\"Error: {str(e)}\\n\")\n",
        "          print(f\"Error for window={window}, timestep={timestep}: {str(e)}\\n\")\n",
        "          traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 LSTM-слой"
      ],
      "metadata": {
        "id": "AFQJMPgOeWmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm3Y9PyqpLOM"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/Thesis/Results\n",
        "all_steps = None\n",
        "all_results = None\n",
        "log_file = \"/content/drive/MyDrive/Thesis/Results/multiclass_log_v1_4h.txt\"\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"Training Log - \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
        "\n",
        "run_multiclass_experiment(path, 0, 1)\n",
        "\n",
        "for window in range(15, 46, 15):\n",
        "    for timestep in range(2, 9):\n",
        "      run_multiclass_experiment(path, window, timestep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wy64NVM7Lfe"
      },
      "source": [
        "4 LSTM-слоя"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_wwLmGNkFhwA"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/Thesis/Results\n",
        "all_steps = None\n",
        "all_results = None\n",
        "log_file = \"/content/drive/MyDrive/Thesis/Results/multiclass_log_v1_1h.txt\"\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"Training Log - \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
        "\n",
        "run_multiclass_experiment(path, 0, 1, \"LSTM4\")\n",
        "\n",
        "for window in range(15, 46, 15):\n",
        "    for timestep in range(2, 9):\n",
        "      run_multiclass_experiment(path, window, timestep, \"LSTM4\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}